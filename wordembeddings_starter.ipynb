{"cells":[{"cell_type":"markdown","metadata":{"id":"UjJn2R-Ft8Gi"},"source":["For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n","\n","Names: Andrew Lin, Praneeth Prathi, Zibing Zhang"]},{"cell_type":"markdown","metadata":{"id":"lzwlPkjgt8Gq"},"source":["Step 1: Train your own word embeddings\n","--------------------------------\n","\n","**TODO**: ADD TO DESCRIPTIONS, general properties, etc.\n","\n","<!-- (describe the provided dataset that you have chosen here) -->\n","The first dataset that we are using was provided by the instructors.\n","It is the complete works of Shakespeare.\n","It seems to have already gone through a large amount of normalization, as there are not punctuation marks, and everything is lowercased.\n","This poses some issue since we do not know sentence boundaries, but we will treat separate lines as separate sentences.\n","\n","<!-- Describe what data set you have chosen to compare and contrast with the your chosen provided dataset. Make sure to describe where it comes from and it's general properties. -->\n","\n","<!-- (describe your dataset here) -->\n","The second dataset that we are using is from the nltk gutenberg corpus.\n","We have chosen the King James version of the Bible.\n","The data is already split by whitespace and punctuation."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T03:27:00.340250Z","start_time":"2020-10-24T03:26:59.570883Z"},"id":"uQLg8dGdt8Gr"},"outputs":[],"source":["# import your libraries here\n","\n","import nltk\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# setup datasets and pre-process into desired format\n","\n","# Read the file and prepare the training data \n","# so that it is in the following format\n","\n","# data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n","# \t\t\t['this', 'is', 'the', 'second', 'sentence'],\n","# \t\t\t['yet', 'another', 'sentence'],\n","# \t\t\t['one', 'more', 'sentence'],\n","# \t\t\t['and', 'the', 'final', 'sentence']]\n","\n","START_TOKEN = \"<s>\"\n","END_TOKEN = \"</s>\"\n","\n","# shakespeare, given dataset\n","dataset_1 = []\n","with open(\"shakespeare_plays.txt\") as f:\n","  dataset_1 = [[START_TOKEN] + line.split() + [END_TOKEN] for line in f.read().strip().split(\"\\n\")]\n","# print(dataset_1[0])  # first sentence\n","\n","# king james version of bible, our datset\n","dataset_2 = []\n","nltk.download(\"gutenberg\")\n","kjv = nltk.corpus.gutenberg.words(\"bible-kjv.txt\")\n","# print(len(kjv))  # 1010654 words\n","sentence = [START_TOKEN]\n","for word in kjv:\n","  # lower case words\n","  word = word.lower()\n","  # period denotes end of sentence\n","  if word == \".\":\n","    sentence.append(END_TOKEN)\n","    dataset_2.append(sentence)\n","    sentence = [START_TOKEN]\n","  # only allow words that contain only letters\n","  elif not re.match(r\"^[a-z]+$\", word):\n","    continue\n","  else:\n","    sentence.append(word)\n","print(dataset_2[0])  # first sentence"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# code to train word embeddings\n","\n","from gensim.models import Word2Vec\n","\n","# The dimension of word embedding. \n","# This variable will be used throughout the program\n","# you may vary this as you desire\n","EMBEDDINGS_SIZE = 200\n","\n","# Train the Word2Vec model from Gensim.\n","# The default arguments are the hyperparameters that are most relevant. \n","# But feel free to explore other \n","# options too:\n","def train_embeddings(dataset, embeddings_size=EMBEDDINGS_SIZE, sg=1, window=5, min_count=1):\n","  return Word2Vec(sentences=dataset, vector_size=embeddings_size, sg=sg, window=window, min_count=min_count)\n"]},{"cell_type":"markdown","metadata":{"id":"mj0A0mCkt8Gt"},"source":["### a) Train embeddings on GIVEN dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T04:39:38.482701Z","start_time":"2020-10-24T04:39:28.044970Z"},"id":"Od_L53GEt8Gv"},"outputs":[],"source":["model_1 = train_embeddings(dataset_1)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T04:39:43.448249Z","start_time":"2020-10-24T04:39:43.444835Z"},"id":"xrt52ahnt8Gw"},"outputs":[],"source":["# if you save your Word2Vec as the variable model, this will \n","# print out the vocabulary size\n","# print('Vocab size {}'.format(len(model.wv.vocab)))  # outdatted in gensim 4.0.0\n","print('Vocab size {}'.format(len(model_1.wv.index_to_key)))"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T04:39:48.730304Z","start_time":"2020-10-24T04:39:45.451960Z"},"id":"UUanXgQLt8Gy"},"outputs":[],"source":["# You can save file in txt format, then load later if you wish.\n","# model_1.wv.save_word2vec_format('embeddings.txt', binary=False)"]},{"cell_type":"markdown","metadata":{"id":"uGMUTMcmt8G0"},"source":["### b) Train embedding on YOUR dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sN0KvmUKt8G0"},"outputs":[],"source":["model_2 = train_embeddings(dataset_2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# if you save your Word2Vec as the variable model, this will \n","# print out the vocabulary size\n","# print('Vocab size {}'.format(len(model.wv.vocab)))  # outdatted in gensim 4.0.0\n","print('Vocab size {}'.format(len(model_2.wv.index_to_key)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# You can save file in txt format, then load later if you wish.\n","# model_2.wv.save_word2vec_format('embeddings.txt', binary=False)"]},{"cell_type":"markdown","metadata":{"id":"BsjzTVFjt8G1"},"source":["__What text-normalization and pre-processing did you do and why?__\n","\n","For the given dataset, there was not any pre-processing that was done, because the dataset was already in a format that was very much well suited to modeling.\n","Everything was lower cased, and there was no punctuation.\n","This did pose some problem, as there was no clear delimiter for when a sentence would begin and end, but we decided to split the corpus into sentences on newlines.\n","\n","The King James Version of the Bible required some text-normalization and pre-processing.\n","The corpus was already split into tokens, but we filtered some out and added some text-normalization.\n","To be overly cautious, we discarded all tokens that contained more than just alphabetic characters.\n","We also lower cased all the words."]},{"cell_type":"markdown","metadata":{"id":"aOFmHpH8t8G2"},"source":["Step 2: Evaluate the differences between the word embeddings\n","----------------------------\n","\n","(make sure to include graphs, figures, and paragraphs with full sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXjy2-OqgvIf"},"outputs":[],"source":["def distance(wv1, wv2):\n","  d = 0\n","  for x1, x2 in zip(wv1, wv2):\n","    d += (x1 - x2) ** 2\n","  return d ** 0.5\n","\n","# i think distance and angle, we probably only need one, since i'd imagine the vectors are somehow normalized to distance 1, unsure..., should check\n","def length(wv):\n","  return sum(e ** 2 for e in wv) ** 0.5\n","def angle(wv1, wv2):\n","  import math\n","  return math.acos(sum(e1 * e2 for e1, e2 in zip(wv1, wv2)) / (length(wv1) * length(wv2)))\n","\n","# maybe compare common words between the two?\n","# for word in [\"apple\", \"boat\", \"god\", \"jesus\", \"happy\", \"sad\", \"sword\", \"good\", \"well\"]:\n","#   print(distance(model_1.wv[word], model_2.wv[word]))\n","\n","print(distance(model_1.wv[\"kitchen\"], model_2.wv[\"woman\"]))\n","print(distance(model_1.wv[\"kitchen\"], model_2.wv[\"life\"]))\n","\n","print(angle(model_1.wv[\"kitchen\"], model_2.wv[\"woman\"]))\n","print(angle(model_1.wv[\"kitchen\"], model_2.wv[\"life\"]))"]},{"cell_type":"markdown","metadata":{"id":"CWlWydbrgv4P"},"source":["##Write down your analysis:"]},{"cell_type":"markdown","metadata":{"id":"-tmrTVDqt8G2"},"source":["Cite your sources:\n","-------------"]},{"cell_type":"markdown","metadata":{"id":"ix2On6zat8G2"},"source":["Step 3: Feedforward Neural Language Model\n","--------------------------"]},{"cell_type":"markdown","metadata":{"id":"AZsCKQWDt8G2"},"source":["### a) First, encode  your text into integers"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-26T21:39:09.625031Z","start_time":"2020-10-26T21:39:09.009109Z"},"id":"ec0KKYj0t8G3"},"outputs":[],"source":["# Importing utility functions from Keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import SimpleRNN\n","from keras.layers import Embedding\n","\n","# The size of the ngram language model you want to train\n","# change as needed for your experiments\n","NGRAM = 3 \n","\n","# Initializing a Tokenizer\n","# It is used to vectorize a text corpus. Here, it just creates a mapping from \n","# word to a unique index. (Note: Indexing starts from 0)\n","# Example:\n","# tokenizer = Tokenizer()\n","# tokenizer.fit_on_texts(data)\n","# encoded = tokenizer.texts_to_sequences(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T03:27:54.373208Z","start_time":"2020-10-24T03:27:54.369835Z"},"id":"U1PrwlBAt8G5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"mCndArPmt8G5"},"source":["### b) Next, prepare your sequences from text"]},{"cell_type":"markdown","metadata":{"id":"6jG42_9Xt8G6"},"source":["#### Fixed ngram based sequences "]},{"cell_type":"raw","metadata":{"id":"HsoPVS8ct8G7"},"source":["The training samples will be structured in the following format. \n","Depending on which ngram model we choose, there will be (n-1) tokens \n","in the input sequence (X) and we will need to predict the nth token (Y)\n","\n","            X,\t\t\t\t\t\t  y\n","    this,    process               however\n","    process, however               afforded\n","    however, afforded\t           me"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T05:21:28.039381Z","start_time":"2020-10-24T05:21:24.941885Z"},"id":"B_4YqhKTt8G7"},"outputs":[],"source":["def generate_ngram_training_samples(ngram: list) -> list:\n","    '''\n","    Takes the encoded data (list of lists) and \n","    generates the training samples out of it.\n","    Parameters:\n","    up to you, we've put in what we used\n","    but you can add/remove as needed\n","    return: \n","    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n","    '''\n","    pass\n"]},{"cell_type":"markdown","metadata":{"id":"BWL6Czlxt8G8"},"source":["### c) Then, split the sequences into X and y and create a Data Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T05:21:31.213422Z","start_time":"2020-10-24T05:21:31.061759Z"},"id":"csweN-d1t8G9"},"outputs":[],"source":["# Note here that the sequences were in the form: \n","# sequence = [x1, x2, ... , x(n-1), y]\n","# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T05:21:34.675827Z","start_time":"2020-10-24T05:21:33.315288Z"},"id":"Vjr6vwP5t8G9"},"outputs":[],"source":["def read_embeddings():\n","    '''Loads and parses embeddings trained in earlier.\n","    Parameters and return values are up to you.\n","    '''\n","    \n","    # you may find generating the following two dicts useful:\n","    # word to embedding : {'the':[0....], ...}\n","    # index to embedding : {1:[0....], ...} \n","    # use your tokenizer's word_index to find the index of\n","    # a given word\n","    pass\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T05:22:24.016237Z","start_time":"2020-10-24T05:22:24.011220Z"},"id":"H6g9g7p6t8G9"},"outputs":[],"source":["def data_generator(X: list, y: list, num_sequences_per_batch: int) -> (list,list):\n","    '''\n","    Returns data generator to be used by feed_forward\n","    https://wiki.python.org/moin/Generators\n","    https://realpython.com/introduction-to-python-generators/\n","    \n","    Yields batches of embeddings and labels to go with them.\n","    Use one hot vectors to encode the labels \n","    (see the to_categorical function)\n","    \n","    '''\n","    pass\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T05:22:55.470133Z","start_time":"2020-10-24T05:22:55.398259Z"},"id":"vgXSWdlMt8G-"},"outputs":[],"source":["# Examples\n","# initialize data_generator\n","# num_sequences_per_batch = 128 # this is the batch size\n","# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n","# train_generator = data_generator(X, y, num_sequences_per_batch)\n","\n","# sample=next(train_generator) # this is how you get data out of generators\n","# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n","# sample[1].shape   # (batch_size, |V|) to_categorical"]},{"cell_type":"markdown","metadata":{"id":"yzfweqz1t8G-"},"source":["### d) Train your models"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T04:56:19.207252Z","start_time":"2020-10-24T04:56:19.204894Z"},"id":"4fZlHukVt8G_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T03:56:50.919869Z","start_time":"2020-10-24T03:56:50.779792Z"},"id":"KmgNnQj5t8G_"},"outputs":[],"source":["# code to train a feedforward neural language model \n","# on a set of given word embeddings\n","# make sure not to just copy + paste to train your two models\n","\n","# Define the model architecture using Keras Sequential API\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T04:01:50.757170Z","start_time":"2020-10-24T03:56:53.620836Z"},"id":"-dWf2qO3t8G_"},"outputs":[],"source":["# Start training the model\n","model.fit(x=train_generator, \n","          steps_per_epoch=steps_per_epoch,\n","          epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVjtknkVt8HA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCZ2S5mpt8HA"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3QwRhKYwt8HA"},"source":["### e) Generate Sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T04:13:54.425934Z","start_time":"2020-10-24T04:13:54.418616Z"},"id":"ewR5ueOJt8HB"},"outputs":[],"source":["# generate a sequence from the model\n","def generate_seq(model: Sequential, \n","                 tokenizer: Tokenizer, \n","                 seed: list, \n","                 n_words: int):\n","    '''\n","    Parameters:\n","        model: your neural network\n","        tokenizer: the keras preprocessing tokenizer\n","        seed: [w1, w2, w(n-1)]\n","        n_words: generate a sentence of length n_words\n","    Returns: string sentence\n","    '''\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-24T04:14:13.123529Z","start_time":"2020-10-24T04:14:13.000264Z"},"id":"XZ9fShSyt8HB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"w68JVS2jt8HB"},"source":["### f) Compare your generated sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xE4dcQdut8HC"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"yet5p8N1t8HC"},"source":["Sources Cited\n","----------------------------\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sPu_1h2t8HC"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"wordembeddings_starter.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
