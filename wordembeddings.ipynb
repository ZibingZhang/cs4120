{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjJn2R-Ft8Gi"
   },
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: __Andrew Lin, Praneeth Prathi, Zibing Zhang__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwlPkjgt8Gq"
   },
   "source": [
    "Step 1: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "<!-- (describe the provided dataset that you have chosen here) -->\n",
    "The first dataset that we are using was provided by the instructors.\n",
    "It is the complete works of Shakespeare.\n",
    "It seems to have already gone through a large amount of normalization, as there are not punctuation marks, and everything is lowercased.\n",
    "This poses some issue since we do not know sentence boundaries, but we will treat separate lines as separate sentences.\n",
    "As Shakespeare's works are mostly plays, there are a lot of stage instructions included in the corpus, as well as lots of proper nouns that only appear for the duration of a specific play.\n",
    "\n",
    "<!-- Describe what data set you have chosen to compare and contrast with the your chosen provided dataset. Make sure to describe where it comes from and it's general properties. -->\n",
    "\n",
    "<!-- (describe your dataset here) -->\n",
    "The second dataset that we are using is from the nltk gutenberg corpus.\n",
    "We have chosen the full text of the King James version of the Bible.\n",
    "The data is already split by whitespace and punctuation.\n",
    "There are a lot of words that indicate the current verse, among other markings.\n",
    "Most of these things are filtered out in text pre-processing as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    },
    "id": "uQLg8dGdt8Gr"
   },
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Zibing\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'the', 'king', 'james', 'bible', 'the', 'old', 'testament', 'of', 'the', 'king', 'james', 'bible', 'the', 'first', 'book', 'of', 'moses', 'called', 'genesis', 'in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the', 'earth', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# setup datasets and pre-process into desired format\n",
    "\n",
    "# Read the file and prepare the training data \n",
    "# so that it is in the following format\n",
    "\n",
    "# data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "# \t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "# \t\t\t['yet', 'another', 'sentence'],\n",
    "# \t\t\t['one', 'more', 'sentence'],\n",
    "# \t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "\n",
    "# shakespeare, given dataset\n",
    "dataset_1 = []\n",
    "with open(\"shakespeare_plays.txt\") as f:\n",
    "  dataset_1 = [[START_TOKEN] + line.split() + [END_TOKEN] for line in f.read().strip().split(\"\\n\")]\n",
    "# print(dataset_1[0])  # first sentence\n",
    "\n",
    "# king james version of bible, our datset\n",
    "dataset_2 = []\n",
    "nltk.download(\"gutenberg\")\n",
    "kjv = nltk.corpus.gutenberg.words(\"bible-kjv.txt\")\n",
    "# print(len(kjv))  # 1010654 words\n",
    "sentence = [START_TOKEN]\n",
    "for word in kjv:\n",
    "  # lower case words\n",
    "  word = word.lower()\n",
    "  # period denotes end of sentence\n",
    "  if word == \".\":\n",
    "    sentence.append(END_TOKEN)\n",
    "    dataset_2.append(sentence)\n",
    "    sentence = [START_TOKEN]\n",
    "  # only allow words that contain only letters\n",
    "  elif not re.match(r\"^[a-z]+$\", word):\n",
    "    continue\n",
    "  else:\n",
    "    sentence.append(word)\n",
    "print(dataset_2[0])  # first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to train word embeddings\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Train the Word2Vec model from Gensim.\n",
    "# The default arguments are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "def train_embeddings(dataset, embeddings_size=EMBEDDINGS_SIZE, sg=1, window=5, min_count=1):\n",
    "  return Word2Vec(sentences=dataset, vector_size=embeddings_size, sg=sg, window=window, min_count=min_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj0A0mCkt8Gt"
   },
   "source": [
    "### a) Train embeddings on GIVEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    },
    "id": "Od_L53GEt8Gv"
   },
   "outputs": [],
   "source": [
    "model_1 = train_embeddings(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:43.448249Z",
     "start_time": "2020-10-24T04:39:43.444835Z"
    },
    "id": "xrt52ahnt8Gw"
   },
   "outputs": [],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "# print('Vocab size {}'.format(len(model.wv.vocab)))  # outdatted in gensim 4.0.0\n",
    "print('Vocab size {}'.format(len(model_1.wv.index_to_key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    },
    "id": "UUanXgQLt8Gy"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "# model_1.wv.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGMUTMcmt8G0"
   },
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sN0KvmUKt8G0"
   },
   "outputs": [],
   "source": [
    "model_2 = train_embeddings(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "# print('Vocab size {}'.format(len(model.wv.vocab)))  # outdatted in gensim 4.0.0\n",
    "print('Vocab size {}'.format(len(model_2.wv.index_to_key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "# model_2.wv.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsjzTVFjt8G1"
   },
   "source": [
    "__What text-normalization and pre-processing did you do and why?__\n",
    "\n",
    "For the given dataset, there was not any pre-processing that was done, because the dataset was already in a format that was very much well suited to modeling.\n",
    "Everything was lower cased, and there was no punctuation.\n",
    "This did pose some problem, as there was no clear delimiter for when a sentence would begin and end, but we decided to split the corpus into sentences on newlines.\n",
    "\n",
    "The King James Version of the Bible required some text-normalization and pre-processing.\n",
    "The corpus was already split into tokens, but we filtered some out and added some text-normalization.\n",
    "To be overly cautious, we discarded all tokens that contained more than just alphabetic characters.\n",
    "We also lower cased all the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOFmHpH8t8G2"
   },
   "source": [
    "Step 2: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NXjy2-OqgvIf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zibing\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Zibing\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Zibing\\Personal\\Northeastern University\\Programming Assignments\\CS 4120\\homework-04\\wordembeddings.ipynb Cell 16'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zibing/Personal/Northeastern%20University/Programming%20Assignments/CS%204120/homework-04/wordembeddings.ipynb#ch0000015?line=18'>19</a>\u001b[0m   embeddings_2d \u001b[39m=\u001b[39m tsne_2d\u001b[39m.\u001b[39mfit_transform(embeddings)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zibing/Personal/Northeastern%20University/Programming%20Assignments/CS%204120/homework-04/wordembeddings.ipynb#ch0000015?line=20'>21</a>\u001b[0m   \u001b[39m# plt.figure(figsize=FIG_SIZE)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zibing/Personal/Northeastern%20University/Programming%20Assignments/CS%204120/homework-04/wordembeddings.ipynb#ch0000015?line=21'>22</a>\u001b[0m   \u001b[39m# plt.scatter(x=embeddings_2d[:,0], y=embeddings_2d[:,1], alpha=1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zibing/Personal/Northeastern%20University/Programming%20Assignments/CS%204120/homework-04/wordembeddings.ipynb#ch0000015?line=22'>23</a>\u001b[0m   \u001b[39m# plt.show()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zibing/Personal/Northeastern%20University/Programming%20Assignments/CS%204120/homework-04/wordembeddings.ipynb#ch0000015?line=27'>28</a>\u001b[0m plot(model_1)\n",
      "\u001b[1;32mc:\\Users\\Zibing\\Personal\\Northeastern University\\Programming Assignments\\CS 4120\\homework-04\\wordembeddings.ipynb Cell 16'\u001b[0m in \u001b[0;36mplot\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zibing/Personal/Northeastern%20University/Programming%20Assignments/CS%204120/homework-04/wordembeddings.ipynb#ch0000015?line=16'>17</a>\u001b[0m embeddings \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mwv[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mindex_to_key)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zibing/Personal/Northeastern%20University/Programming%20Assignments/CS%204120/homework-04/wordembeddings.ipynb#ch0000015?line=17'>18</a>\u001b[0m tsne_2d \u001b[39m=\u001b[39m TSNE(perplexity\u001b[39m=\u001b[39mPERPLEXITY, n_components\u001b[39m=\u001b[39mN_COMPONENTS, init\u001b[39m=\u001b[39mINIT, n_iter\u001b[39m=\u001b[39mN_ITER, random_state\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zibing/Personal/Northeastern%20University/Programming%20Assignments/CS%204120/homework-04/wordembeddings.ipynb#ch0000015?line=18'>19</a>\u001b[0m embeddings_2d \u001b[39m=\u001b[39m tsne_2d\u001b[39m.\u001b[39;49mfit_transform(embeddings)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1108\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1087'>1088</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1088'>1089</a>\u001b[0m     \u001b[39m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1089'>1090</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1090'>1091</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1105'>1106</a>\u001b[0m \u001b[39m        Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1106'>1107</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1107'>1108</a>\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1108'>1109</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ \u001b[39m=\u001b[39m embedding\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1004\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=997'>998</a>\u001b[0m \u001b[39m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=998'>999</a>\u001b[0m \u001b[39m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=999'>1000</a>\u001b[0m \u001b[39m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1000'>1001</a>\u001b[0m \u001b[39m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1001'>1002</a>\u001b[0m degrees_of_freedom \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m-> <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1003'>1004</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tsne(\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1004'>1005</a>\u001b[0m     P,\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1005'>1006</a>\u001b[0m     degrees_of_freedom,\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1006'>1007</a>\u001b[0m     n_samples,\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1007'>1008</a>\u001b[0m     X_embedded\u001b[39m=\u001b[39;49mX_embedded,\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1008'>1009</a>\u001b[0m     neighbors\u001b[39m=\u001b[39;49mneighbors_nn,\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1009'>1010</a>\u001b[0m     skip_num_points\u001b[39m=\u001b[39;49mskip_num_points,\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1010'>1011</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1056\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[1;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1052'>1053</a>\u001b[0m \u001b[39m# Learning schedule (part 1): do 250 iteration with lower momentum but\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1053'>1054</a>\u001b[0m \u001b[39m# higher learning rate controlled via the early exaggeration parameter\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1054'>1055</a>\u001b[0m P \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mearly_exaggeration\n\u001b[1;32m-> <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1055'>1056</a>\u001b[0m params, kl_divergence, it \u001b[39m=\u001b[39m _gradient_descent(obj_func, params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopt_args)\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1056'>1057</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1057'>1058</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1058'>1059</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m[t-SNE] KL divergence after \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m iterations with early exaggeration: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1059'>1060</a>\u001b[0m         \u001b[39m%\u001b[39m (it \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, kl_divergence)\n\u001b[0;32m   <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=1060'>1061</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:398\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[1;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=394'>395</a>\u001b[0m \u001b[39m# only compute the error when needed\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=395'>396</a>\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mcompute_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m check_convergence \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m n_iter \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=397'>398</a>\u001b[0m error, grad \u001b[39m=\u001b[39m objective(p, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=398'>399</a>\u001b[0m grad_norm \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mnorm(grad)\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=400'>401</a>\u001b[0m inc \u001b[39m=\u001b[39m update \u001b[39m*\u001b[39m grad \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:279\u001b[0m, in \u001b[0;36m_kl_divergence_bh\u001b[1;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=275'>276</a>\u001b[0m indptr \u001b[39m=\u001b[39m P\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=277'>278</a>\u001b[0m grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(X_embedded\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=278'>279</a>\u001b[0m error \u001b[39m=\u001b[39m _barnes_hut_tsne\u001b[39m.\u001b[39;49mgradient(\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=279'>280</a>\u001b[0m     val_P,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=280'>281</a>\u001b[0m     X_embedded,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=281'>282</a>\u001b[0m     neighbors,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=282'>283</a>\u001b[0m     indptr,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=283'>284</a>\u001b[0m     grad,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=284'>285</a>\u001b[0m     angle,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=285'>286</a>\u001b[0m     n_components,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=286'>287</a>\u001b[0m     verbose,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=287'>288</a>\u001b[0m     dof\u001b[39m=\u001b[39;49mdegrees_of_freedom,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=288'>289</a>\u001b[0m     compute_error\u001b[39m=\u001b[39;49mcompute_error,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=289'>290</a>\u001b[0m     num_threads\u001b[39m=\u001b[39;49mnum_threads,\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=290'>291</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=291'>292</a>\u001b[0m c \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m (degrees_of_freedom \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m/\u001b[39m degrees_of_freedom\n\u001b[0;32m    <a href='file:///c%3A/Users/Zibing/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/manifold/_t_sne.py?line=292'>293</a>\u001b[0m grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39mravel()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TSNE parameters\n",
    "PERPLEXITY = 5\n",
    "N_COMPONENTS = 2\n",
    "INIT = \"pca\"\n",
    "N_ITER = 250  # 3500\n",
    "RANDOM_STATE = 32\n",
    "\n",
    "# figure parameters\n",
    "FIG_SIZE = 15, 10\n",
    "\n",
    "# libraries\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(model):\n",
    "  # https://umap-learn.readthedocs.io/en/latest/embedding_space.html\n",
    "  embeddings = [model.wv[word] for word in list(model.wv.index_to_key)]\n",
    "  tsne_2d = TSNE(perplexity=PERPLEXITY, n_components=N_COMPONENTS, init=INIT, n_iter=N_ITER, random_state=32)\n",
    "  embeddings_2d = tsne_2d.fit_transform(embeddings)\n",
    "\n",
    "  # plt.figure(figsize=FIG_SIZE)\n",
    "  # plt.scatter(x=embeddings_2d[:,0], y=embeddings_2d[:,1], alpha=1)\n",
    "  # plt.show()\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "plot(model_1)\n",
    "# plot(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWlWydbrgv4P"
   },
   "source": [
    "## Write down your analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tmrTVDqt8G2"
   },
   "source": [
    "Cite your sources:\n",
    "-------------\n",
    "- https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix2On6zat8G2"
   },
   "source": [
    "Step 3: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZsCKQWDt8G2"
   },
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    },
    "id": "ec0KKYj0t8G3"
   },
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import SimpleRNN\n",
    "# from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the ngram language model you want to train\n",
    "# change as needed for your experiments\n",
    "NGRAM = 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not sure if this function should be adding all sent tokens to front and none to end or the way that it is now because of how step b is supposed to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentence_tokens(ngram, dataset):\n",
    "    if ngram < 3:\n",
    "        return dataset\n",
    "    \n",
    "    prepared_dataset = [[(ngram - 2) * START_TOKEN] + sentence + [(ngram - 2) * END_TOKEN] for sentence in dataset]\n",
    "    return prepared_dataset\n",
    "\n",
    "dataset_1_ngrams = add_sentence_tokens(NGRAM, dataset_1)\n",
    "dataset_2_ngrams = add_sentence_tokens(NGRAM, dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:54.373208Z",
     "start_time": "2020-10-24T03:27:54.369835Z"
    },
    "id": "U1PrwlBAt8G5"
   },
   "outputs": [],
   "source": [
    "def encode_text(data):\n",
    "    # Initializing a Tokenizer\n",
    "    # It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "    # word to a unique index. (Note: Indexing starts from 0)\n",
    "    # Example:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    encoded = tokenizer.texts_to_sequences(data)\n",
    "    return encoded\n",
    "\n",
    "encoded_text_1 = encode_text(dataset_1_ngrams)\n",
    "encoded_text_2 = encode_text(dataset_2_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCndArPmt8G5"
   },
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jG42_9Xt8G6"
   },
   "source": [
    "#### Fixed ngram based sequences "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HsoPVS8ct8G7"
   },
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process               however\n",
    "    process, however               afforded\n",
    "    however, afforded\t           me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    },
    "id": "B_4YqhKTt8G7"
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded_data: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    training_samples = []\n",
    "    \n",
    "    for sentence in encoded_data:\n",
    "        for i in range(len(sentence) - (ngram - 1)):\n",
    "            training_samples.append(sentence[i:i+ngram])\n",
    "    \n",
    "    return training_samples\n",
    "    \n",
    "training_samples_1 = generate_ngram_training_samples(encoded_text_1, 3)\n",
    "training_samples_2 = generate_ngram_training_samples(encoded_text_2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWL6Czlxt8G8"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:31.213422Z",
     "start_time": "2020-10-24T05:21:31.061759Z"
    },
    "id": "csweN-d1t8G9"
   },
   "outputs": [],
   "source": [
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(training_samples: list) -> list:\n",
    "    X = np.array(training_samples)[:,:-1]\n",
    "    y = np.array(training_samples)[:,-1]\n",
    "    return X, y\n",
    "    \n",
    "X1, y1 = split_sequences(training_samples_1)\n",
    "X2, y2 = split_sequences(training_samples_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    },
    "id": "Vjr6vwP5t8G9"
   },
   "outputs": [],
   "source": [
    "def read_embeddings():\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    '''\n",
    "    \n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':[0....], ...}\n",
    "    # index to embedding : {1:[0....], ...} \n",
    "    # use your tokenizer's word_index to find the index of\n",
    "    # a given word\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    },
    "id": "H6g9g7p6t8G9"
   },
   "outputs": [],
   "source": [
    "def data_generator(X: list, y: list, num_sequences_per_batch: int) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:55.470133Z",
     "start_time": "2020-10-24T05:22:55.398259Z"
    },
    "id": "vgXSWdlMt8G-"
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "# initialize data_generator\n",
    "# num_sequences_per_batch = 128 # this is the batch size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzfweqz1t8G-"
   },
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:56:19.207252Z",
     "start_time": "2020-10-24T04:56:19.204894Z"
    },
    "id": "4fZlHukVt8G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    },
    "id": "KmgNnQj5t8G_"
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "\n",
    "# Define the model architecture using Keras Sequential API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "id": "-dWf2qO3t8G_"
   },
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "model.fit(x=train_generator, \n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVjtknkVt8HA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCZ2S5mpt8HA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QwRhKYwt8HA"
   },
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    },
    "id": "ewR5ueOJt8HB"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list, \n",
    "                 n_words: int):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:14:13.123529Z",
     "start_time": "2020-10-24T04:14:13.000264Z"
    },
    "id": "XZ9fShSyt8HB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w68JVS2jt8HB"
   },
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE4dcQdut8HC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yet5p8N1t8HC"
   },
   "source": [
    "Sources Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sPu_1h2t8HC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wordembeddings_starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
